# vitoquoc.py
from __future__ import annotations
import re, time
from pathlib import Path
from urllib.parse import urlparse, urljoin
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

FIXED_LABEL_PATHS = [
    "search/label/B%C3%ACnh%20lu%E1%BA%ADn",
    "search/label/Ch%C3%ADnh%20tr%E1%BB%8B%20-%20X%C3%A3%20h%E1%BB%99i",
    "search/label/Bi%C3%AAn%20gi%E1%BB%9Bi%20-%20Bi%E1%BB%83n%20%C4%91%E1%BA%A3o",
    "search/label/Th%E1%BA%BF%20Gi%E1%BB%9Bi",
    "search/label/Cu%E1%BB%99c%20s%E1%BB%91ng",
    "search/label/Th%E1%BA%BF%20gi%E1%BB%9Bi%20tr%E1%BA%BB",
    "search/label/v%C4%83n%20h%C3%B3a",
    "search/label/G%C3%B3c%20th%C6%B0%20gi%C3%A3n",
]
LABEL_CATEGORIES = {
    "search/label/B%C3%ACnh%20lu%E1%BA%ADn": "B√¨nh Lu·∫≠n",
    "search/label/Ch%C3%ADnh%20tr%E1%BB%8B%20-%20X%C3%A3%20h%E1%BB%99i": "Ch√≠nh tr·ªã - X√£ h·ªôi",
    "search/label/Bi%C3%AAn%20gi%E1%BB%9Bi%20-%20Bi%E1%BB%83n%20%C4%91%E1%BA%A3o": "Bi√™n gi·ªõi - Bi·ªÉn ƒë·∫£o",
    "search/label/Th%E1%BA%BF%20Gi%E1%BB%9Bi": "Th·∫ø Gi·ªõi",
    "search/label/Cu%E1%BB%99c%20s%E1%BB%91ng": "Cu·ªôc s·ªëng",
    "search/label/Th%E1%BA%BF%20gi%E1%BB%9Bi%20tr%E1%BA%BB": "Th·∫ø gi·ªõi tr·∫ª",
    "search/label/v%C4%83n%20h%C3%B3a": "VƒÉn h√≥a",
    "search/label/G%C3%B3c%20th%C6%B0%20gi%C3%A3n": "G√≥c th∆∞ gi√£n",
}
WAIT_SECS = 10

# ===================== SELECTORS =====================
SELECTOR_LIST_LINKS = (
    "h2.post-title a[href], h3.post-title a[href], "
    "div.post h2 a[href], h2 > a[href]"
)

# N√∫t sang trang c≈© h∆°n (next)
NEXT_SELECTORS = [
    "a.blog-pager-older-link",
    "span.blog-pager-older-link a",
    ".blog-pager-older-link a",
    "a.older-link",
    "a.older-posts",
]

# ===================== HELPERS =====================
def _wait_body(driver, secs: int = WAIT_SECS):
    WebDriverWait(driver, secs).until(EC.presence_of_element_located((By.TAG_NAME, "body")))

def _normalize_base(base_url: str) -> str:
    base_url = (base_url or "").strip()
    if not base_url:
        return ""
    if "://" not in base_url:
        base_url = "https://" + base_url
    if not base_url.endswith("/"):
        base_url += "/"
    return base_url

def _root_from_url(u: str) -> str:
    try:
        p = urlparse(u)
        return f"{p.scheme}://{p.netloc}/" if p.scheme and p.netloc else ""
    except:
        return ""

# ===================== CONTENT =====================
def extract_content(driver, *, label_url: str = "", category_name: str = "", data_source_root: str = "") -> dict:
    """
    Tr·∫£ v·ªÅ schema chu·∫©n:
      dataSource, title, url, author, publishedDate, content, contentImagesUrls, categories
    """
    article_url = driver.current_url
    if not data_source_root:
        data_source_root = _root_from_url(label_url or article_url)

    if not category_name and label_url:
        # N·∫øu c√≥ mapping LABEL_CATEGORIES, d√πng n√≥:
        try:
            path = urlparse(label_url).path
            for k, v in LABEL_CATEGORIES.items():
                if path.endswith(k):
                    category_name = v
                    break
        except:
            pass

    title = author = publishTime = ""
    content, images = "", []

    # --- title ---
    try:
        el = driver.find_element(By.CSS_SELECTOR, "h1, h2.post-title, h2.art-postheader, h1.entry-title, div.post h2")
        title = (el.text or "").strip()
    except:
        pass

    # --- date ---
    try:
        date = driver.find_element(By.CSS_SELECTOR, "div.time-view i, time.published, p.MsoNoSpacing span, abbr.published, div.postmeta-primary span.meta_date")
        publishTime = (date.text or "").strip()
    except:
        pass

    # --- content & images ---
    try:
        div = driver.find_element(By.CSS_SELECTOR, "div.post-body, div.post-info")
        content = (div.text or "").strip()
        for img in div.find_elements(By.TAG_NAME, "img"):
            src = img.get_attribute("src")
            if src and src not in images:
                images.append(src)
        try:
            # T√°ch theo d√≤ng v√† l·∫•y d√≤ng cu·ªëi c√πng c√≥ ch·ªØ
            lines = [ln.strip() for ln in content.splitlines() if ln.strip()]
            if lines:
                last_line = lines[-1]
                if len(last_line.split()) <= 3:
                    author = last_line
        except:
            pass
    except:
        pass

    return {
        "dataSource": data_source_root,
        "title": title,
        "url": article_url,
        "author": author,
        "publishedDate": publishTime,
        "content": content,
        "contentImagesUrls": images,
        "categories": category_name or "",
    }

# ===================== LISTING HELPERS =====================
def _collect_links_on_listing(driver) -> list[str]:
    """L·∫•y c√°c link b√†i t·ª´ trang listing/label hi·ªán t·∫°i (l·ªçc tr√πng trong trang)."""
    _wait_body(driver)
    links, seen = [], set()
    for a in driver.find_elements(By.CSS_SELECTOR, SELECTOR_LIST_LINKS):
        h = (a.get_attribute("href") or "").strip()
        if not h or not h.startswith("http"):
            continue
        if h not in seen:
            seen.add(h)
            links.append(h)
    return links

def _find_next_url(driver) -> str | None:
    """T√¨m URL trang 'c≈© h∆°n' (next) v·ªõi nhi·ªÅu bi·∫øn th·ªÉ selector."""
    for sel in NEXT_SELECTORS:
        try:
            el = driver.find_element(By.CSS_SELECTOR, sel)
            href = (el.get_attribute("href") or "").strip()
            if href.startswith("http"):
                return href
        except:
            continue
    return None

# ===================== CRAWL 1 LABEL (URL tuy·ªát ƒë·ªëi) =====================
def crawl_label(driver, label_url: str, category_name: str = "", on_record=None):
    driver.get(label_url)
    _wait_body(driver)
    results = []
    visited_listing = set()
    seen_links_global = set()
    page = 0

    while True:
        cur = driver.current_url.split("#", 1)[0]
        if cur in visited_listing: break
        visited_listing.add(cur); page += 1
        print(f"üìÑ ƒêang crawl trang{page}: {cur}")

        links = _collect_links_on_listing(driver)
        links = [u for u in links if u not in seen_links_global]
        for u in links: seen_links_global.add(u)
        print(f"  + {len(links)} b√†i")

        for i, link in enumerate(links, 1):
            try:
                listing = driver.current_window_handle
                driver.execute_script("window.open(arguments[0], '_blank');", link)
                WebDriverWait(driver, WAIT_SECS).until(lambda d: len(d.window_handles) > 1)
                newh = [h for h in driver.window_handles if h != listing][0]
                driver.switch_to.window(newh)

                _wait_body(driver)

                item = extract_content(
                    driver,
                    label_url=label_url,
                    category_name=category_name,
                    data_source_root=_root_from_url(label_url),
                )
                results.append(item)

                if on_record:
                    on_record(item)

            except Exception as e:
                print(f"   üî¥ l·ªói b√†i: {e}")
            finally:
                try:
                    driver.close(); driver.switch_to.window(listing)
                except:
                    pass

        nxt = _find_next_url(driver)
        if not nxt: print("‚úÖ h·∫øt trang."); break
        driver.get(nxt)

def crawl_fixed_labels(driver, base_url: str, out_dir: Path | None = None,
                       on_record=None, max_pages_per_label: int = 100):
    """
    Gh√©p base_url + c√°c path label c·ªë ƒë·ªãnh ‚Üí l·∫ßn l∆∞·ª£t crawl t·ª´ng label.
    """
    base = _normalize_base(base_url)
    label_urls = [urljoin(base, p) for p in FIXED_LABEL_PATHS]

    if out_dir:
        out_dir.mkdir(parents=True, exist_ok=True)

    for label_url in label_urls:
        category_name = LABEL_CATEGORIES.get(label_url.replace(base, "").strip("/"), "Unknown")
        crawl_label(
            driver,
            label_url,
            out_path = None,
            category_name=category_name,
            on_record=on_record,
            max_pages=max_pages_per_label
        )

# ===================== OVERRIDE: CRAWL TO√ÄN DOMAIN = labels =====================
def crawl_one_domain(driver, start_url: str, on_record=None, max_pages=500):
    """
    ƒê∆∞·ª£c g·ªçi b·ªüi crawl_manager.py (mode=domain).
    Thay v√¨ crawl homepage, H√ÄM N√ÄY CHUY·ªÇN H∆Ø·ªöNG sang crawl theo c√°c label c·ªë ƒë·ªãnh.
    - start_url: domain g·ªëc, vd: https://vitoquocvietnam2012.blogspot.com/
    - out_path: file m√† crawl_manager k·ª≥ v·ªçng; ta s·∫Ω d√πng th∆∞ m·ª•c cha c·ªßa n√≥ ƒë·ªÉ l∆∞u per-label.
    """
    # D√πng flow labels, kh√¥ng crawl homepage
    crawl_fixed_labels(
        driver,
        base_url=start_url,
        out_dir= None,
        on_record=on_record,
        max_pages_per_label=max_pages
    )   

# === th√™m v√†o cu·ªëi vitoquoc.py (ngay d∆∞·ªõi crawl_fixed_labels) ===
def crawl_from_root(driver, root_url: str, out_dir: Path | None = None,
                    on_record=None, max_pages_per_label: int = 100):
    """
    Nh·∫≠n domain ch√≠nh (v√≠ d·ª• https://vitoquocvietnam2012.blogspot.com/)
    r·ªìi t·ª± ƒë·ªông crawl t·∫•t c·∫£ label c·ªë ƒë·ªãnh trong FIXED_LABEL_PATHS.
    """
    return crawl_fixed_labels(
        driver,
        base_url=root_url,
        out_dir=out_dir,
        on_record=on_record,
        max_pages_per_label=max_pages_per_label,
    )
